{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What an efficient algorithm should have\n",
    "- It should be as specific as possible\n",
    "- It should have each instruction properly defined\n",
    "- There should not be any ambiguous instruction\n",
    "- All the instructions in the algorithm should be executed in finite amount of time in finite number of steps\n",
    "- It should have clear input and output to solve the problem\n",
    "- Each instruction of the algorithm should be integral in solving given problem\n",
    "\n",
    "## Things to keep in mind when defining the algorithm\n",
    "- The algorithm should be correct and should produce the results as expected for all input values\n",
    "- The algorithm should be optimal in the sense that it should be executed on the computer within the desired time limit, in line with a optimal memory space requirement\n",
    "\n",
    "## Performance Analysis of algorithm\n",
    "<strong> The performance of the algorithm is measured by the size of its input data 'n', and the time ad the memory space used by the algorithm </strong>\n",
    "- The time required is measured by the key operations to be performed by the algorithm (such as comparison operation ). Because the key operations are the instructions that takes significant amount of time.\n",
    "- The space requirement is measured by the memory needed to store the variable, constants, and instructions during the execution of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Time Complexity \n",
    "- The time complexity of the algorithm is the amount of time that an algorithm will take to execute on a computer system to produce an output.\n",
    "- The running time required by an algorithm depends on inputs size 'n'. If the size of the input is big the run time of the algorithm will be high. \n",
    "\n",
    "### Worst case running time\n",
    "**The worst-case running time of the algorithm is the upper-bound complexity, it is the maximum runtime required for an algorithm to execute for any given input.**\n",
    "- This parameter is very useful because it guarantees that for any given inputs the time taken to execute the algorithm will not cross this worst-case running time\n",
    "### Best case running time\n",
    "**This best case running time is the minimum time needed for an algorithm to run**\n",
    "### Average case running time\n",
    "**The average case running time is the average running time required for an algorithm to execute.**\n",
    "- In general probabilistic analysis is used to analyze average run time. Where averaging is calculated over the distribution of all the possible inputs. \n",
    "\n",
    "**However in real world scenario the worst case running time is mostly used as it guarantees that the running time will not take any longer than this**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space Complexity\n",
    "- The space complexity estimates the memory requirement to execute it on a computer to produce the output as a function of input data. \n",
    "- Given two algorithm to solve the problem considering all the parameters are same, which ever the algorithm which consumes less space will be taken.\n",
    "**- The space complexity is measured in O(n) where n is the input.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asymptotic Notation\n",
    " - Asymptotic analysis is the way that we analyze the efficiency of the algorithm for large input sizes considering the higher order of growth ad ignoring the multiplicative constants and lower order ones. To analyze time complexity of an algorithm is the rate of growth(order of growth) is very important when the input size is large.\n",
    " - We compare two algorithm with respect to the input size rather than the actual run time and measure how the time take increases with an increased input size. The algorithm which is considered more efficient asymptotically is generally considered a better algorithm as compared to the other algorithm.\n",
    " - The following asymptotic notations are commonly used to calculate the running time of time complexity of an algorithm \n",
    "    - θ notation - It denotes the worst case running time complexity with a tight bound\n",
    "    - O notation - It denotes the worst case running tme complexity with an upper bound, which ensures that the function never grows faster tha the upper bound\n",
    "    - Ω notation - It denotes the lower bound of an algorithm running time. It measures the best amount of time to execute the algorithm\n",
    "\n",
    "### Theta Notation (θ)\n",
    " - Theta notation denotes the worst case running time for an algorithm with a tight bound. \n",
    " - For the given function F(n) the asymptotic worst case running time complexity can be defined as \n",
    "      - **T(n) = θ(F(n))**\n",
    "      - If there is a constant n0, c1 and c2\n",
    "      - ![If there exist constant n0, c1 and c2 such that](./images/theta_formula.png)\n",
    "        - The function T(n) belongs to a set of functions ϴ(F(n)) if there exists positive constants c1 and c2 such that the value of T(n) always lies in between c1F(n) and c2F(n) for all large values of n. If this condition is true, then we say F(n) is asymptotically tight bound for T(n).\n",
    " - Example theta calculation\n",
    "    - ![Example 1](./images/theta_calculation.png)\n",
    "    - ![Example 2](./images/theta_calculation_eg2.png)\n",
    "\n",
    "### Big O Notation(O)\n",
    "  - It denotes the worst case running tme complexity with an upper bound, which ensures that the function never grows faster tha the upper bound\n",
    "  - Given F(n), the T(n) is a Big O function of F(n) and we define this as \n",
    "    - **T(n)=O(F(n))**\n",
    "    - If there exist a constant then n0 and c such that **T(n) <= c(F(n)) for all n >= 0**\n",
    "  - In big O notation a constant multiple of F(n) is a asymptotic upper bound on T(n), and the positive constants n0 and c should be in such a way that all values of n greater than n0 always lie on or below function C*F(n)\n",
    "  - Moreover we only care what happens at the higher value of . The variable n0 represents the threshold below which the rate of growth is not important\n",
    "  - In the following table we list the most common growth rates in order from lowest to highest\n",
    "    - ![Time_Complexity_O](./images/time_complexity_of_O.png)\n",
    "  \n",
    "### Omega Notation (Ω)\n",
    "  - It denotes the asymptotic lower bound of the algorithm.\n",
    "  - It computes the best case runtime complexity of the algorithm\n",
    "  - The Ω notation (Ω(F(n)) is pronounced as omega of F of n), is a set of functions in such a way that there are positive constants n0 and c such that for all values of n greater than n0, T(n) always lies on or above a function to c*F(n).\n",
    "    - T(n) = Ω (F(n))\n",
    "    - ![Omega_Notation](./images/omega_notitation.png)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amortized Analysis\n",
    "- In amortized analysis of an algorithm, we average the time required to execute a sequence of operation with all the operation of the algorithm.\n",
    "- Amortized analysis is important when we are not interested in the time complexity of individual operations but we are interested in the average runtime of sequences of operations. \n",
    "- Amortized analysis gives us the average performance of the each operation in the worst case.\n",
    "- There are three commonly used methods in amortized analysis\n",
    "    - Aggregate Analysis\n",
    "    - Accounting Method\n",
    "    - Potential Method\n",
    "\n",
    "### Aggregate Analysis\n",
    "- In this the amortized cost is the average cost of sequence of operation\n",
    "- For a given sequence of n operations, the amortized cost of each operation can be computed by dividing the upper bound on the total cost of n operations with n.\n",
    "\n",
    "### The Accounting Method\n",
    "- In this method we assign an amortized cost to each operation which may be different than their actual cost. \n",
    "- In this, we impose an extra charge on early operations in the sequence and save “credit cost,” which is used to pay expensive operations later in the sequence.\n",
    "\n",
    "### The Potential Method\n",
    "- It is similar to the accounting method where we impose extra charge on early operation in the sequence which will be used as a credit at later expense, but the Potential method accumulates the overcharged credit as \"potential energy\" of the data structure as a whole intead of storing credit for individual operations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing Complexity Classes\n",
    "\n",
    "- To calculate the complexity of a while loop, we multiple the complexity classes by the number of times the operation is carried out. \n",
    "- For the for loop lets say that the loop executes n times, then time complexity of the code will be\n",
    "    - O(n2) X O(n) = O(n X n2) = O(n3)\n",
    "    - Here we are multiplying the time complexity of the inner function with the number of times this function executes. \n",
    "    - The runtime of the loop is the most the runtime of the statement inside the loop and the number of times it gets executed\n",
    "    - A single nested loop will run n2 times.\n",
    "- We can define base2 logarithmic complexity reducing the size of the problem by half, in constant time. \n",
    "    - k = log(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Find the time complexity of the following Python snippets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. \n",
    "\n",
    "i=1\n",
    "while(i<n):\n",
    "    i*=2\n",
    "    print(\"data\")\n",
    "\n",
    "# Answer: Since the I is multipllied twice the O(logn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "\n",
    "i =n\n",
    "while(i>0):\n",
    "    print('complexity')\n",
    "    i/ = 2\n",
    "\n",
    "# Answer: As we are dividing the integer i by 2 in each step there will be exactly log(n) steps. (n, n/2, n/4, …… till 1). So answer is O(logn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.\n",
    "for i in range(1,n):\n",
    "    j = i\n",
    "    while(j<n):\n",
    "        j*=2\n",
    "\n",
    "# Answer: Since the outer loop executes n times and inner executes log(n) times the O is O(nlogn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. \n",
    "\n",
    "i=1\n",
    "while(i<n):\n",
    "    print('python')\n",
    "        i = i**2\n",
    "    \n",
    "\n",
    "# n this code snippet, the while loop will execute based on the value of i until the condition becomes false. The value of i is incrementing in the following series: 2, 4, 16, 256, ... n. We can see that the number of times the loop is executing is log2(log2(n)) for a given value of n. So, for this series there will be exactly log2(log2(n)) executions of the loop. Hence the time complexity will be \n",
    "# O(log2(log2(n)).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
